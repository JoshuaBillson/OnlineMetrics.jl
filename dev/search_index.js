var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = OnlineMetrics","category":"page"},{"location":"#OnlineMetrics","page":"Home","title":"OnlineMetrics","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for OnlineMetrics.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [OnlineMetrics]","category":"page"},{"location":"#OnlineMetrics.AbstractMetric","page":"Home","title":"OnlineMetrics.AbstractMetric","text":"Metrics are measures of a model's performance, such as loss, accuracy, or squared error.\n\nEach metric must implement the following interface:\n\nstep!(metric, ŷ, y): Updates the metric state for the given batch of predictions and labels.\nvalue(metric): Return the current value of the metric.\nparams(metric): Return a NamedTuple containing any internal parameters you wish to report.\n\nExample Implementation\n\nmutable struct Accuracy <: ClassificationMetric\n    correct::Int\n    n::Int\n    Accuracy() = new(0,0)\nend\n\nfunction step!(m::Accuracy, ŷ::AbstractVector{<:Integer}, y::AbstractVector{<:Integer})\n    m.correct += sum(ŷ .== y)\n    m.n += length(ŷ)\nend\n\nvalue(m::Accuracy) = m.correct / max(m.n, 1)\n\nparams(x::Accuracy) = (;x.n, x.correct)\n\n\n\n\n\n","category":"type"},{"location":"#OnlineMetrics.Accuracy","page":"Home","title":"OnlineMetrics.Accuracy","text":"Accuracy()\n\nMeasures the model's overall accuracy as correct / n.\n\n\n\n\n\n","category":"type"},{"location":"#OnlineMetrics.AverageMeasure","page":"Home","title":"OnlineMetrics.AverageMeasure","text":"Loss(loss::Function)\n\nTracks the average model loss as total_loss / steps\n\n\n\n\n\n","category":"type"},{"location":"#OnlineMetrics.ClassificationMetric","page":"Home","title":"OnlineMetrics.ClassificationMetric","text":"Classification metrics are used to evaluate the performance of models that predict a discrete label for each observation. Subtypes should implement an batchstate method which assumes that both ŷ and y are encoded as integers.\n\n\n\n\n\n","category":"type"},{"location":"#OnlineMetrics.ConfusionMatrix","page":"Home","title":"OnlineMetrics.ConfusionMatrix","text":"ConfusionMatrix(nclasses::Int)\n\nCalculate the confusion matrix over two or more classes. The columns of the resulting nclasses x nclasses matrix correspond to the true label while the rows correspond to the prediction.\n\nArguments\n\nnclasses::Int: The number of possible classes in the classification task.\n\n\n\n\n\n","category":"type"},{"location":"#OnlineMetrics.MIoU","page":"Home","title":"OnlineMetrics.MIoU","text":"MIoU(nclasses::Int)\n\nMean Intersection over Union (MIoU) is a measure of the overlap between a prediction and a label. This measure is frequently used for segmentation models.\n\n\n\n\n\n","category":"type"},{"location":"#OnlineMetrics.MetricCollection","page":"Home","title":"OnlineMetrics.MetricCollection","text":"MetricCollection(metrics...)\n\nAn object to track one or more metrics concurrently.\n\nExample\n\njulia> mc = MetricCollection(Accuracy(), MIoU(2));\n\njulia> step!(mc, [0, 0, 1, 0], [0, 0, 1, 1]);\n\njulia> mc\nMetricCollection\n├─ Accuracy\n│  ├─ :value ⇒ 0.75\n│  ├─ :n ⇒ 4\n│  └─ :correct ⇒ 3\n└─ MIoU\n   ├─ :value ⇒ 0.583333\n   ├─ :union ⇒ [3, 2]\n   └─ :intersection ⇒ [2, 1]\n\n\n\n\n\n","category":"type"},{"location":"#OnlineMetrics.Precision","page":"Home","title":"OnlineMetrics.Precision","text":"Precision(nclasses::Int; agg=:macro)\n\nPrecision is the ratio of true positives to the sum of true positives and false positives, measuring the accuracy of positive predictions.\n\nArguments\n\nclasses::Vector{Int}: A vector of integer class labels representing the possible classes for the classification task.\n\nKeyword Arguments\n\nagg: Specifies the type of precision aggregation to be computed. The possible values are:\n:macro: Calculates macro-averaged precision, which computes the precision for each class independently and then takes the average.\n:micro: Calculates micro-averaged precision, which aggregates the contributions of all classes to compute a single precision value.\n:nothing: Calculates the per-class precision, which is returned as a Vector with the same length as classes.\n\n\n\n\n\n","category":"type"},{"location":"#OnlineMetrics.Recall","page":"Home","title":"OnlineMetrics.Recall","text":"Recall(nclasses::Int; agg=:macro)\n\nRecall, also known as sensitivity or true positive rate, is the ratio of true positives to the sum of true positives and false negatives, measuring the ability of the classifier to identify all positive instances.\n\nArguments\n\nclasses::Vector{Int}: A vector of integer class labels representing the possible classes for the classification task.\n\nKeyword Arguments\n\nagg: Specifies the type of recall aggregation to be computed. The possible values are:\n:macro: Calculates macro-averaged recall, which computes the recall for each class independently and then takes the average.\n:micro: Calculates micro-averaged recall, which aggregates the contributions of all classes to compute a single recall value.\n:nothing: Calculates the per-class recall, which is returned as a Vector with the same length as classes.\n\n\n\n\n\n","category":"type"},{"location":"#OnlineMetrics.params-Tuple{AbstractMetric}","page":"Home","title":"OnlineMetrics.params","text":"params(m::AbstractMetric)\n\nReturns any internal parameters used to track the metric's current value.\n\n\n\n\n\n","category":"method"},{"location":"#OnlineMetrics.step!","page":"Home","title":"OnlineMetrics.step!","text":"step!(m::AbstractMetric, ŷ, y)\n\nUpdate the metric state for the given batch of labels and predictions.\n\n\n\n\n\n","category":"function"},{"location":"#OnlineMetrics.value","page":"Home","title":"OnlineMetrics.value","text":"value(m::AbstractMetric)\n\nCompute the performance measure from the current metric state.\n\n\n\n\n\n","category":"function"}]
}
